{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "primes = []\n",
    "firstLine = True\n",
    "with open(\"primes1.txt\", 'r') as fobj:\n",
    "    for line in fobj:\n",
    "        if firstLine:\n",
    "            firstLine = False\n",
    "            continue\n",
    "        numbers = [int(num) for num in line.split()]\n",
    "        primes.extend(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes = np.array(primes)\n",
    "bit_count = 64\n",
    "primes_bit = (((primes[:,None] & (1 << np.arange(bit_count)))) > 0).astype(int)\n",
    "primes_bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.arange(len(primes))\n",
    "inputs_bit = (((inputs[:,None] & (1 << np.arange(bit_count)))) > 0).astype(int)\n",
    "inputs_bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 16.50110636\n",
      "Iteration 2, loss = 13.90434417\n",
      "Iteration 3, loss = 12.84605734\n",
      "Iteration 4, loss = 12.39789664\n",
      "Iteration 5, loss = 12.01578972\n",
      "Iteration 6, loss = 11.77967094\n",
      "Iteration 7, loss = 11.50000883\n",
      "Iteration 8, loss = 11.32404560\n",
      "Iteration 9, loss = 11.17561526\n",
      "Iteration 10, loss = 10.95235670\n",
      "Iteration 11, loss = 10.79913842\n",
      "Iteration 12, loss = 10.70765984\n",
      "Iteration 13, loss = 10.65101255\n",
      "Iteration 14, loss = 10.59296602\n",
      "Iteration 15, loss = 10.50331416\n",
      "Iteration 16, loss = 10.40241234\n",
      "Iteration 17, loss = 10.28475531\n",
      "Iteration 18, loss = 10.17955797\n",
      "Iteration 19, loss = 10.11349295\n",
      "Iteration 20, loss = 10.01885862\n",
      "Iteration 21, loss = 10.01423499\n",
      "Iteration 22, loss = 9.95991693\n",
      "Iteration 23, loss = 9.85970941\n",
      "Iteration 24, loss = 9.81071778\n",
      "Iteration 25, loss = 9.72962309\n",
      "Iteration 26, loss = 9.68452193\n",
      "Iteration 27, loss = 9.62304908\n",
      "Iteration 28, loss = 9.54390119\n",
      "Iteration 29, loss = 9.47909704\n",
      "Iteration 30, loss = 9.44299636\n",
      "Iteration 31, loss = 9.41073431\n",
      "Iteration 32, loss = 9.41068554\n",
      "Iteration 33, loss = 9.36956597\n",
      "Iteration 34, loss = 9.31463850\n",
      "Iteration 35, loss = 9.35052600\n",
      "Iteration 36, loss = 9.30123605\n",
      "Iteration 37, loss = 9.26020787\n",
      "Iteration 38, loss = 9.22267874\n",
      "Iteration 39, loss = 9.24751023\n",
      "Iteration 40, loss = 9.18111105\n",
      "Iteration 41, loss = 9.12021328\n",
      "Iteration 42, loss = 9.12161957\n",
      "Iteration 43, loss = 9.10573842\n",
      "Iteration 44, loss = 9.00961386\n",
      "Iteration 45, loss = 9.00471855\n",
      "Iteration 46, loss = 8.95828768\n",
      "Iteration 47, loss = 8.95858263\n",
      "Iteration 48, loss = 8.88069276\n",
      "Iteration 49, loss = 8.86495020\n",
      "Iteration 50, loss = 8.83390094\n",
      "Iteration 51, loss = 8.78370282\n",
      "Iteration 52, loss = 8.79642814\n",
      "Iteration 53, loss = 8.75480515\n",
      "Iteration 54, loss = 8.72985351\n",
      "Iteration 55, loss = 8.69774399\n",
      "Iteration 56, loss = 8.68267097\n",
      "Iteration 57, loss = 8.69370643\n",
      "Iteration 58, loss = 8.65077855\n",
      "Iteration 59, loss = 8.68892454\n",
      "Iteration 60, loss = 8.63297201\n",
      "Iteration 61, loss = 8.57233788\n",
      "Iteration 62, loss = 8.58813308\n",
      "Iteration 63, loss = 8.57092693\n",
      "Iteration 64, loss = 8.53500418\n",
      "Iteration 65, loss = 8.58255661\n",
      "Iteration 66, loss = 8.49834500\n",
      "Iteration 67, loss = 8.49924232\n",
      "Iteration 68, loss = 8.47428777\n",
      "Iteration 69, loss = 8.50295602\n",
      "Iteration 70, loss = 8.44993802\n",
      "Iteration 71, loss = 8.42675932\n",
      "Iteration 72, loss = 8.38879646\n",
      "Iteration 73, loss = 8.37746555\n",
      "Iteration 74, loss = 8.42849073\n",
      "Iteration 75, loss = 8.42201493\n",
      "Iteration 76, loss = 8.34655991\n",
      "Iteration 77, loss = 8.30095593\n",
      "Iteration 78, loss = 8.31462021\n",
      "Iteration 79, loss = 8.23635591\n",
      "Iteration 80, loss = 8.26753975\n",
      "Iteration 81, loss = 8.19608263\n",
      "Iteration 82, loss = 8.27706755\n",
      "Iteration 83, loss = 8.19179843\n",
      "Iteration 84, loss = 8.20653557\n",
      "Iteration 85, loss = 8.18415727\n",
      "Iteration 86, loss = 8.16622031\n",
      "Iteration 87, loss = 8.06185380\n",
      "Iteration 88, loss = 8.11399858\n",
      "Iteration 89, loss = 8.11296582\n",
      "Iteration 90, loss = 8.06840495\n",
      "Iteration 91, loss = 8.05702618\n",
      "Iteration 92, loss = 8.09212201\n",
      "Iteration 93, loss = 8.04813979\n",
      "Iteration 94, loss = 8.04792224\n",
      "Iteration 95, loss = 8.02524206\n",
      "Iteration 96, loss = 8.02029147\n",
      "Iteration 97, loss = 7.98129833\n",
      "Iteration 98, loss = 8.02401074\n",
      "Iteration 99, loss = 8.00435011\n",
      "Iteration 100, loss = 7.93886738\n",
      "Iteration 101, loss = 7.98962971\n",
      "Iteration 102, loss = 8.01270900\n",
      "Iteration 103, loss = 7.88780843\n",
      "Iteration 104, loss = 7.94727805\n",
      "Iteration 105, loss = 7.89695302\n",
      "Iteration 106, loss = 7.86102166\n",
      "Iteration 107, loss = 7.88487496\n",
      "Iteration 108, loss = 7.91565350\n",
      "Iteration 109, loss = 7.89450566\n",
      "Iteration 110, loss = 7.84373044\n",
      "Iteration 111, loss = 7.84432480\n",
      "Iteration 112, loss = 7.88614894\n",
      "Iteration 113, loss = 7.90070020\n",
      "Iteration 114, loss = 7.79719660\n",
      "Iteration 115, loss = 7.75820593\n",
      "Iteration 116, loss = 7.80589137\n",
      "Iteration 117, loss = 7.82993819\n",
      "Iteration 118, loss = 7.80461917\n",
      "Iteration 119, loss = 7.81455496\n",
      "Iteration 120, loss = 7.80378862\n",
      "Iteration 121, loss = 7.80989524\n",
      "Iteration 122, loss = 7.70372260\n",
      "Iteration 123, loss = 7.73741082\n",
      "Iteration 124, loss = 7.76686157\n",
      "Iteration 125, loss = 7.83570309\n",
      "Iteration 126, loss = 7.69448840\n",
      "Iteration 127, loss = 7.69937276\n",
      "Iteration 128, loss = 7.64995539\n",
      "Iteration 129, loss = 7.70811029\n",
      "Iteration 130, loss = 7.65668882\n",
      "Iteration 131, loss = 7.66090049\n",
      "Iteration 132, loss = 7.69882893\n",
      "Iteration 133, loss = 7.67914032\n",
      "Iteration 134, loss = 7.75770804\n",
      "Iteration 135, loss = 7.60639212\n",
      "Iteration 136, loss = 7.56298754\n",
      "Iteration 137, loss = 7.55678816\n",
      "Iteration 138, loss = 7.60096908\n",
      "Iteration 139, loss = 7.64897401\n",
      "Iteration 140, loss = 7.54555093\n",
      "Iteration 141, loss = 7.57144879\n",
      "Iteration 142, loss = 7.53274416\n",
      "Iteration 143, loss = 7.58596927\n",
      "Iteration 144, loss = 7.58554527\n",
      "Iteration 145, loss = 7.56862038\n",
      "Iteration 146, loss = 7.61557026\n",
      "Iteration 147, loss = 7.54657257\n",
      "Iteration 148, loss = 7.50496803\n",
      "Iteration 149, loss = 7.43924201\n",
      "Iteration 150, loss = 7.43604904\n",
      "Iteration 151, loss = 7.47409074\n",
      "Iteration 152, loss = 7.50234330\n",
      "Iteration 153, loss = 7.40222873\n",
      "Iteration 154, loss = 7.56099556\n",
      "Iteration 155, loss = 7.54826873\n",
      "Iteration 156, loss = 7.39263607\n",
      "Iteration 157, loss = 7.40091952\n",
      "Iteration 158, loss = 7.50936249\n",
      "Iteration 159, loss = 7.48025643\n",
      "Iteration 160, loss = 7.37400498\n",
      "Iteration 161, loss = 7.35353102\n",
      "Iteration 162, loss = 7.38935163\n",
      "Iteration 163, loss = 7.32873151\n",
      "Iteration 164, loss = 7.36885167\n",
      "Iteration 165, loss = 7.35522653\n",
      "Iteration 166, loss = 7.32950133\n",
      "Iteration 167, loss = 7.38456704\n",
      "Iteration 168, loss = 7.38382130\n",
      "Iteration 169, loss = 7.21372761\n",
      "Iteration 170, loss = 7.33802381\n",
      "Iteration 171, loss = 7.33080610\n",
      "Iteration 172, loss = 7.33325230\n",
      "Iteration 173, loss = 7.31130422\n",
      "Iteration 174, loss = 7.31207201\n",
      "Iteration 175, loss = 7.29423651\n",
      "Iteration 176, loss = 7.20231102\n",
      "Iteration 177, loss = 7.23869319\n",
      "Iteration 178, loss = 7.31173607\n",
      "Iteration 179, loss = 7.24731226\n",
      "Iteration 180, loss = 7.26233838\n",
      "Iteration 181, loss = 7.29932552\n",
      "Iteration 182, loss = 7.28231871\n",
      "Iteration 183, loss = 7.28517093\n",
      "Iteration 184, loss = 7.20418087\n",
      "Iteration 185, loss = 7.17283793\n",
      "Iteration 186, loss = 7.21377961\n",
      "Iteration 187, loss = 7.11784531\n",
      "Iteration 188, loss = 7.15271545\n",
      "Iteration 189, loss = 7.17232138\n",
      "Iteration 190, loss = 7.28592222\n",
      "Iteration 191, loss = 7.17349138\n",
      "Iteration 192, loss = 7.14034605\n",
      "Iteration 193, loss = 7.18514155\n",
      "Iteration 194, loss = 7.17688164\n",
      "Iteration 195, loss = 7.07911454\n",
      "Iteration 196, loss = 7.26605198\n",
      "Iteration 197, loss = 7.18000450\n",
      "Iteration 198, loss = 7.01828879\n",
      "Iteration 199, loss = 7.06936884\n",
      "Iteration 200, loss = 7.09320786\n",
      "Iteration 201, loss = 7.16405172\n",
      "Iteration 202, loss = 7.14797330\n",
      "Iteration 203, loss = 7.19786477\n",
      "Iteration 204, loss = 7.06376419\n",
      "Iteration 205, loss = 7.04552494\n",
      "Iteration 206, loss = 7.05065822\n",
      "Iteration 207, loss = 7.15301517\n",
      "Iteration 208, loss = 7.18213818\n",
      "Iteration 209, loss = 7.06402904\n",
      "Iteration 210, loss = 7.07478121\n",
      "Iteration 211, loss = 7.01399683\n",
      "Iteration 212, loss = 6.98414386\n",
      "Iteration 213, loss = 7.05628830\n",
      "Iteration 214, loss = 7.00197156\n",
      "Iteration 215, loss = 7.07561250\n",
      "Iteration 216, loss = 7.08192342\n",
      "Iteration 217, loss = 6.97404468\n",
      "Iteration 218, loss = 7.07348608\n",
      "Iteration 219, loss = 7.10709086\n",
      "Iteration 220, loss = 7.06151966\n",
      "Iteration 221, loss = 6.92016652\n",
      "Iteration 222, loss = 6.93858795\n",
      "Iteration 223, loss = 7.04400489\n",
      "Iteration 224, loss = 7.11632631\n",
      "Iteration 225, loss = 6.99411347\n",
      "Iteration 226, loss = 6.87834928\n",
      "Iteration 227, loss = 6.88176683\n",
      "Iteration 228, loss = 6.98147705\n",
      "Iteration 229, loss = 6.98592007\n",
      "Iteration 230, loss = 6.96036684\n",
      "Iteration 231, loss = 6.93672693\n",
      "Iteration 232, loss = 7.00976883\n",
      "Iteration 233, loss = 6.92385059\n",
      "Iteration 234, loss = 6.89268238\n",
      "Iteration 235, loss = 6.95766823\n",
      "Iteration 236, loss = 7.02829882\n",
      "Iteration 237, loss = 6.85432193\n",
      "Iteration 238, loss = 6.88947418\n",
      "Iteration 239, loss = 6.91919668\n",
      "Iteration 240, loss = 6.88689624\n",
      "Iteration 241, loss = 6.95466241\n",
      "Iteration 242, loss = 6.92896826\n",
      "Iteration 243, loss = 6.83530441\n",
      "Iteration 244, loss = 6.82820943\n",
      "Iteration 245, loss = 6.90405644\n",
      "Iteration 246, loss = 6.95777341\n",
      "Iteration 247, loss = 6.92450413\n",
      "Iteration 248, loss = 6.78089746\n",
      "Iteration 249, loss = 6.82226558\n",
      "Iteration 250, loss = 6.95138028\n",
      "Iteration 251, loss = 6.92213136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 6.77928035\n",
      "Iteration 253, loss = 6.81666261\n",
      "Iteration 254, loss = 6.81991039\n",
      "Iteration 255, loss = 6.79842083\n",
      "Iteration 256, loss = 6.87934972\n",
      "Iteration 257, loss = 6.85202396\n",
      "Iteration 258, loss = 6.82028612\n",
      "Iteration 259, loss = 6.86734297\n",
      "Iteration 260, loss = 6.88198590\n",
      "Iteration 261, loss = 6.82650643\n",
      "Iteration 262, loss = 6.68053414\n",
      "Iteration 263, loss = 6.69984135\n",
      "Iteration 264, loss = 6.73159759\n",
      "Iteration 265, loss = 6.96656133\n",
      "Iteration 266, loss = 6.78122291\n",
      "Iteration 267, loss = 6.84306474\n",
      "Iteration 268, loss = 6.78151304\n",
      "Iteration 269, loss = 6.64901076\n",
      "Iteration 270, loss = 6.67047301\n",
      "Iteration 271, loss = 6.97619111\n",
      "Iteration 272, loss = 6.74847490\n",
      "Iteration 273, loss = 6.73538872\n",
      "Iteration 274, loss = 6.71503743\n",
      "Iteration 275, loss = 6.64716108\n",
      "Iteration 276, loss = 6.75612093\n",
      "Iteration 277, loss = 6.76466571\n",
      "Iteration 278, loss = 6.73918658\n",
      "Iteration 279, loss = 6.79528756\n",
      "Iteration 280, loss = 6.81158399\n",
      "Iteration 281, loss = 6.67132534\n",
      "Iteration 282, loss = 6.61692851\n",
      "Iteration 283, loss = 6.79146066\n",
      "Iteration 284, loss = 6.76723707\n",
      "Iteration 285, loss = 6.69826554\n",
      "Iteration 286, loss = 6.62863097\n",
      "Iteration 287, loss = 6.53907246\n",
      "Iteration 288, loss = 6.69675228\n",
      "Iteration 289, loss = 6.92867514\n",
      "Iteration 290, loss = 6.70040552\n",
      "Iteration 291, loss = 6.62045656\n",
      "Iteration 292, loss = 6.54047516\n",
      "Iteration 293, loss = 6.56456550\n",
      "Iteration 294, loss = 6.56622256\n",
      "Iteration 295, loss = 6.71106757\n",
      "Iteration 296, loss = 6.85233504\n",
      "Iteration 297, loss = 6.70191653\n",
      "Iteration 298, loss = 6.63660553\n",
      "Iteration 299, loss = 6.52554287\n",
      "Iteration 300, loss = 6.59898325\n",
      "Iteration 301, loss = 6.65059755\n",
      "Iteration 302, loss = 6.54463035\n",
      "Iteration 303, loss = 6.62638649\n",
      "Iteration 304, loss = 6.72430882\n",
      "Iteration 305, loss = 6.73044670\n",
      "Iteration 306, loss = 6.71004648\n",
      "Iteration 307, loss = 6.48384079\n",
      "Iteration 308, loss = 6.54249271\n",
      "Iteration 309, loss = 6.63997164\n",
      "Iteration 310, loss = 6.80943749\n",
      "Iteration 311, loss = 6.71575657\n",
      "Iteration 312, loss = 6.46253072\n",
      "Iteration 313, loss = 6.48378091\n",
      "Iteration 314, loss = 6.50868162\n",
      "Iteration 315, loss = 6.46903853\n",
      "Iteration 316, loss = 6.61931843\n",
      "Iteration 317, loss = 6.57463005\n",
      "Iteration 318, loss = 6.62660347\n",
      "Iteration 319, loss = 6.51783773\n",
      "Iteration 320, loss = 6.54953482\n",
      "Iteration 321, loss = 6.73493603\n",
      "Iteration 322, loss = 6.48226816\n",
      "Iteration 323, loss = 6.45020977\n",
      "Iteration 324, loss = 6.47274214\n",
      "Iteration 325, loss = 6.49021613\n",
      "Iteration 326, loss = 6.61317970\n",
      "Iteration 327, loss = 6.45872573\n",
      "Iteration 328, loss = 6.47809742\n",
      "Iteration 329, loss = 6.48718010\n",
      "Iteration 330, loss = 6.54111475\n",
      "Iteration 331, loss = 6.44439207\n",
      "Iteration 332, loss = 6.48608783\n",
      "Iteration 333, loss = 6.52503220\n",
      "Iteration 334, loss = 6.74797778\n",
      "Iteration 335, loss = 6.69301692\n",
      "Iteration 336, loss = 6.45088079\n",
      "Iteration 337, loss = 6.39219295\n",
      "Iteration 338, loss = 6.29339624\n",
      "Iteration 339, loss = 6.42455511\n",
      "Iteration 340, loss = 6.58992916\n",
      "Iteration 341, loss = 6.47211284\n",
      "Iteration 342, loss = 6.47287831\n",
      "Iteration 343, loss = 6.41934606\n",
      "Iteration 344, loss = 6.42819583\n",
      "Iteration 345, loss = 6.47077730\n",
      "Iteration 346, loss = 6.45399772\n",
      "Iteration 347, loss = 6.45722601\n",
      "Iteration 348, loss = 6.35953378\n",
      "Iteration 349, loss = 6.41162370\n",
      "Iteration 350, loss = 6.47957439\n",
      "Iteration 351, loss = 6.43119972\n",
      "Iteration 352, loss = 6.34712242\n",
      "Iteration 353, loss = 6.34770592\n",
      "Iteration 354, loss = 6.50791051\n",
      "Iteration 355, loss = 6.43653644\n",
      "Iteration 356, loss = 6.34525915\n",
      "Iteration 357, loss = 6.47162526\n",
      "Iteration 358, loss = 6.46461814\n",
      "Iteration 359, loss = 6.45585343\n",
      "Iteration 360, loss = 6.40190424\n",
      "Iteration 361, loss = 6.28233941\n",
      "Iteration 362, loss = 6.48985513\n",
      "Iteration 363, loss = 6.35810766\n",
      "Iteration 364, loss = 6.35500191\n",
      "Iteration 365, loss = 6.37318877\n",
      "Iteration 366, loss = 6.39263833\n",
      "Iteration 367, loss = 6.38487926\n",
      "Iteration 368, loss = 6.34693074\n",
      "Iteration 369, loss = 6.29876951\n",
      "Iteration 370, loss = 6.28252415\n",
      "Iteration 371, loss = 6.46142006\n",
      "Iteration 372, loss = 6.39873873\n",
      "Iteration 373, loss = 6.37973217\n",
      "Iteration 374, loss = 6.26478930\n",
      "Iteration 375, loss = 6.23241177\n",
      "Iteration 376, loss = 6.35665343\n",
      "Iteration 377, loss = 6.41720816\n",
      "Iteration 378, loss = 6.38347568\n",
      "Iteration 379, loss = 6.36703434\n",
      "Iteration 380, loss = 6.24787398\n",
      "Iteration 381, loss = 6.26010127\n",
      "Iteration 382, loss = 6.25903161\n",
      "Iteration 383, loss = 6.39737099\n",
      "Iteration 384, loss = 6.39316656\n",
      "Iteration 385, loss = 6.29385026\n",
      "Iteration 386, loss = 6.19923750\n",
      "Iteration 387, loss = 6.21440639\n",
      "Iteration 388, loss = 6.39813827\n",
      "Iteration 389, loss = 6.47597124\n",
      "Iteration 390, loss = 6.29076694\n",
      "Iteration 391, loss = 6.19615779\n",
      "Iteration 392, loss = 6.24092117\n",
      "Iteration 393, loss = 6.21795007\n",
      "Iteration 394, loss = 6.26645594\n",
      "Iteration 395, loss = 6.31771144\n",
      "Iteration 396, loss = 6.39725771\n",
      "Iteration 397, loss = 6.30396677\n",
      "Iteration 398, loss = 6.19630030\n",
      "Iteration 399, loss = 6.19497091\n",
      "Iteration 400, loss = 6.26903342\n",
      "Iteration 401, loss = 6.31269230\n",
      "Iteration 402, loss = 6.25625681\n",
      "Iteration 403, loss = 6.19775224\n",
      "Iteration 404, loss = 6.17265745\n",
      "Iteration 405, loss = 6.21166857\n",
      "Iteration 406, loss = 6.31669983\n",
      "Iteration 407, loss = 6.51613644\n",
      "Iteration 408, loss = 6.31111052\n",
      "Iteration 409, loss = 6.16942311\n",
      "Iteration 410, loss = 6.18697101\n",
      "Iteration 411, loss = 6.12201096\n",
      "Iteration 412, loss = 6.42135652\n",
      "Iteration 413, loss = 6.30373621\n",
      "Iteration 414, loss = 6.16004078\n",
      "Iteration 415, loss = 6.15308016\n",
      "Iteration 416, loss = 6.18203114\n",
      "Iteration 417, loss = 6.20026334\n",
      "Iteration 418, loss = 6.15563078\n",
      "Iteration 419, loss = 6.22798451\n",
      "Iteration 420, loss = 6.20438212\n",
      "Iteration 421, loss = 6.21681721\n",
      "Iteration 422, loss = 6.25812631\n",
      "Iteration 423, loss = 6.24502557\n",
      "Iteration 424, loss = 6.14099114\n",
      "Iteration 425, loss = 6.14674273\n",
      "Iteration 426, loss = 6.25308170\n",
      "Iteration 427, loss = 6.24994078\n",
      "Iteration 428, loss = 6.22586355\n",
      "Iteration 429, loss = 6.22072448\n",
      "Iteration 430, loss = 6.14267696\n",
      "Iteration 431, loss = 6.03175510\n",
      "Iteration 432, loss = 6.16603813\n",
      "Iteration 433, loss = 6.22356454\n",
      "Iteration 434, loss = 6.13706957\n",
      "Iteration 435, loss = 6.11475354\n",
      "Iteration 436, loss = 6.18470468\n",
      "Iteration 437, loss = 6.09621927\n",
      "Iteration 438, loss = 6.08193807\n",
      "Iteration 439, loss = 6.34478373\n",
      "Iteration 440, loss = 6.18397324\n",
      "Iteration 441, loss = 6.02048418\n",
      "Iteration 442, loss = 6.15321054\n",
      "Iteration 443, loss = 6.05744684\n",
      "Iteration 444, loss = 6.10629377\n",
      "Iteration 445, loss = 6.16109337\n",
      "Iteration 446, loss = 6.07732148\n",
      "Iteration 447, loss = 6.19318280\n",
      "Iteration 448, loss = 6.05301481\n",
      "Iteration 449, loss = 6.21461047\n",
      "Iteration 450, loss = 6.15404587\n",
      "Iteration 451, loss = 6.03762631\n",
      "Iteration 452, loss = 6.09930911\n",
      "Iteration 453, loss = 6.13104241\n",
      "Iteration 454, loss = 6.09619895\n",
      "Iteration 455, loss = 6.15032850\n",
      "Iteration 456, loss = 6.08850580\n",
      "Iteration 457, loss = 6.23731843\n",
      "Iteration 458, loss = 6.10135069\n",
      "Iteration 459, loss = 6.03908242\n",
      "Iteration 460, loss = 6.19981431\n",
      "Iteration 461, loss = 6.04311682\n",
      "Iteration 462, loss = 5.97627983\n",
      "Iteration 463, loss = 5.95373665\n",
      "Iteration 464, loss = 6.23404097\n",
      "Iteration 465, loss = 6.15375941\n",
      "Iteration 466, loss = 6.01128976\n",
      "Iteration 467, loss = 6.10868429\n",
      "Iteration 468, loss = 6.19994111\n",
      "Iteration 469, loss = 5.96528065\n",
      "Iteration 470, loss = 5.97503625\n",
      "Iteration 471, loss = 6.03189553\n",
      "Iteration 472, loss = 6.04465762\n",
      "Iteration 473, loss = 5.98611343\n",
      "Iteration 474, loss = 5.99782027\n",
      "Iteration 475, loss = 6.27113098\n",
      "Iteration 476, loss = 6.25508768\n",
      "Iteration 477, loss = 5.93953042\n",
      "Iteration 478, loss = 5.88323801\n",
      "Iteration 479, loss = 6.15582677\n",
      "Iteration 480, loss = 5.97220514\n",
      "Iteration 481, loss = 5.99658923\n",
      "Iteration 482, loss = 6.09586949\n",
      "Iteration 483, loss = 6.10331510\n",
      "Iteration 484, loss = 6.22648350\n",
      "Iteration 485, loss = 5.98290168\n",
      "Iteration 486, loss = 5.89489987\n",
      "Iteration 487, loss = 5.95036708\n",
      "Iteration 488, loss = 6.09052250\n",
      "Iteration 489, loss = 6.08949889\n",
      "Iteration 490, loss = 6.15156373\n",
      "Iteration 491, loss = 5.97680310\n",
      "Iteration 492, loss = 5.95876451\n",
      "Iteration 493, loss = 6.02652749\n",
      "Iteration 494, loss = 6.00958953\n",
      "Iteration 495, loss = 6.00100590\n",
      "Iteration 496, loss = 5.92570636\n",
      "Iteration 497, loss = 5.93131483\n",
      "Iteration 498, loss = 6.09841403\n",
      "Iteration 499, loss = 6.06387854\n",
      "Iteration 500, loss = 5.90318340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 501, loss = 5.86945218\n",
      "Iteration 502, loss = 6.02423571\n",
      "Iteration 503, loss = 6.06737397\n",
      "Iteration 504, loss = 5.96188391\n",
      "Iteration 505, loss = 5.95097663\n",
      "Iteration 506, loss = 5.92470080\n",
      "Iteration 507, loss = 5.93891381\n",
      "Iteration 508, loss = 6.11510991\n",
      "Iteration 509, loss = 5.94051734\n",
      "Iteration 510, loss = 5.91684909\n",
      "Iteration 511, loss = 6.00358813\n",
      "Iteration 512, loss = 5.89580890\n",
      "Iteration 513, loss = 6.09286963\n",
      "Iteration 514, loss = 5.96911686\n",
      "Iteration 515, loss = 5.85914600\n",
      "Iteration 516, loss = 5.89613553\n",
      "Iteration 517, loss = 5.96547512\n",
      "Iteration 518, loss = 5.95433851\n",
      "Iteration 519, loss = 5.98997556\n",
      "Iteration 520, loss = 6.18480927\n",
      "Iteration 521, loss = 5.94507167\n",
      "Iteration 522, loss = 5.80924274\n",
      "Iteration 523, loss = 5.83994068\n",
      "Iteration 524, loss = 5.85437001\n",
      "Iteration 525, loss = 5.90253677\n",
      "Iteration 526, loss = 6.12564272\n",
      "Iteration 527, loss = 5.91694220\n",
      "Iteration 528, loss = 5.88854856\n",
      "Iteration 529, loss = 5.91446337\n",
      "Iteration 530, loss = 5.89103439\n",
      "Iteration 531, loss = 5.92286883\n",
      "Iteration 532, loss = 5.89062507\n",
      "Iteration 533, loss = 5.92262373\n",
      "Iteration 534, loss = 5.87555366\n",
      "Iteration 535, loss = 5.88364861\n",
      "Iteration 536, loss = 5.92635791\n",
      "Iteration 537, loss = 5.87173596\n",
      "Iteration 538, loss = 5.95627576\n",
      "Iteration 539, loss = 5.88570753\n",
      "Iteration 540, loss = 5.82466843\n",
      "Iteration 541, loss = 5.92651901\n",
      "Iteration 542, loss = 5.81212202\n",
      "Iteration 543, loss = 5.90865448\n",
      "Iteration 544, loss = 5.97625280\n",
      "Iteration 545, loss = 5.94185015\n",
      "Iteration 546, loss = 5.95539448\n",
      "Iteration 547, loss = 5.86530206\n",
      "Iteration 548, loss = 5.72916713\n",
      "Iteration 549, loss = 5.76078744\n",
      "Iteration 550, loss = 6.02317063\n",
      "Iteration 551, loss = 5.93318455\n",
      "Iteration 552, loss = 5.75225363\n",
      "Iteration 553, loss = 5.83679892\n",
      "Iteration 554, loss = 5.85184775\n",
      "Iteration 555, loss = 5.73747047\n",
      "Iteration 556, loss = 5.82604934\n",
      "Iteration 557, loss = 5.85362347\n",
      "Iteration 558, loss = 5.99560170\n",
      "Iteration 559, loss = 5.96266198\n",
      "Iteration 560, loss = 5.83823241\n",
      "Iteration 561, loss = 5.71021089\n",
      "Iteration 562, loss = 5.78393121\n",
      "Iteration 563, loss = 5.79109333\n",
      "Iteration 564, loss = 5.81563869\n",
      "Iteration 565, loss = 5.83367101\n",
      "Iteration 566, loss = 5.82475318\n",
      "Iteration 567, loss = 5.87179058\n",
      "Iteration 568, loss = 5.92639137\n",
      "Iteration 569, loss = 5.90548641\n",
      "Iteration 570, loss = 5.73506313\n",
      "Iteration 571, loss = 5.83110844\n",
      "Iteration 572, loss = 5.86005951\n",
      "Iteration 573, loss = 5.78050604\n",
      "Iteration 574, loss = 5.94319819\n",
      "Iteration 575, loss = 5.75686620\n",
      "Iteration 576, loss = 5.77998569\n",
      "Iteration 577, loss = 5.64385148\n",
      "Iteration 578, loss = 5.71199311\n",
      "Iteration 579, loss = 5.69494013\n",
      "Iteration 580, loss = 5.80146465\n",
      "Iteration 581, loss = 5.92993944\n",
      "Iteration 582, loss = 5.81690894\n",
      "Iteration 583, loss = 5.87528933\n",
      "Iteration 584, loss = 5.82688273\n",
      "Iteration 585, loss = 5.74585061\n",
      "Iteration 586, loss = 5.66550559\n",
      "Iteration 587, loss = 5.69131662\n",
      "Iteration 588, loss = 5.78352827\n",
      "Iteration 589, loss = 5.85998658\n",
      "Iteration 590, loss = 5.86404979\n",
      "Iteration 591, loss = 5.83827142\n",
      "Iteration 592, loss = 5.72752021\n",
      "Iteration 593, loss = 5.71523508\n",
      "Iteration 594, loss = 5.77981809\n",
      "Iteration 595, loss = 5.73125197\n",
      "Iteration 596, loss = 5.74559946\n",
      "Iteration 597, loss = 5.66689702\n",
      "Iteration 598, loss = 5.83432292\n",
      "Iteration 599, loss = 5.87088861\n",
      "Iteration 600, loss = 5.70874304\n",
      "Iteration 601, loss = 5.71301616\n",
      "Iteration 602, loss = 5.76409992\n",
      "Iteration 603, loss = 5.71518285\n",
      "Iteration 604, loss = 5.81063765\n",
      "Iteration 605, loss = 5.79179419\n",
      "Iteration 606, loss = 5.65147077\n",
      "Iteration 607, loss = 5.68657758\n",
      "Iteration 608, loss = 5.68921602\n",
      "Iteration 609, loss = 5.78162576\n",
      "Iteration 610, loss = 5.90320914\n",
      "Iteration 611, loss = 5.87217967\n",
      "Iteration 612, loss = 5.79795473\n",
      "Iteration 613, loss = 5.70931836\n",
      "Iteration 614, loss = 5.65459094\n",
      "Iteration 615, loss = 5.60215207\n",
      "Iteration 616, loss = 5.64705198\n",
      "Iteration 617, loss = 5.73903723\n",
      "Iteration 618, loss = 5.85583704\n",
      "Iteration 619, loss = 5.80994059\n",
      "Iteration 620, loss = 5.62878254\n",
      "Iteration 621, loss = 5.63477373\n",
      "Iteration 622, loss = 5.65825042\n",
      "Iteration 623, loss = 5.71144337\n",
      "Iteration 624, loss = 5.71861776\n",
      "Iteration 625, loss = 5.68766200\n",
      "Iteration 626, loss = 5.79104465\n",
      "Iteration 627, loss = 5.68146764\n",
      "Iteration 628, loss = 5.63395229\n",
      "Iteration 629, loss = 5.64086895\n",
      "Iteration 630, loss = 5.72814539\n",
      "Iteration 631, loss = 5.80181771\n",
      "Iteration 632, loss = 5.60742386\n",
      "Iteration 633, loss = 5.71341839\n",
      "Iteration 634, loss = 5.65823523\n",
      "Iteration 635, loss = 5.61779521\n",
      "Iteration 636, loss = 5.61803280\n",
      "Iteration 637, loss = 5.86169585\n",
      "Iteration 638, loss = 5.82120912\n",
      "Iteration 639, loss = 5.64470900\n",
      "Iteration 640, loss = 5.57582991\n",
      "Iteration 641, loss = 5.57505677\n",
      "Iteration 642, loss = 5.81851585\n",
      "Iteration 643, loss = 5.65829813\n",
      "Iteration 644, loss = 5.53437828\n",
      "Iteration 645, loss = 5.65891640\n",
      "Iteration 646, loss = 5.68116555\n",
      "Iteration 647, loss = 5.64579685\n",
      "Iteration 648, loss = 5.66195050\n",
      "Iteration 649, loss = 5.64711323\n",
      "Iteration 650, loss = 5.58243590\n",
      "Iteration 651, loss = 5.71050879\n",
      "Iteration 652, loss = 5.65341727\n",
      "Iteration 653, loss = 5.54747692\n",
      "Iteration 654, loss = 5.49113181\n",
      "Iteration 655, loss = 5.62190019\n",
      "Iteration 656, loss = 5.97673824\n",
      "Iteration 657, loss = 5.86101930\n",
      "Iteration 658, loss = 5.52969892\n",
      "Iteration 659, loss = 5.43408225\n",
      "Iteration 660, loss = 5.65990914\n",
      "Iteration 661, loss = 5.66807058\n",
      "Iteration 662, loss = 5.63412835\n",
      "Iteration 663, loss = 5.59711360\n",
      "Iteration 664, loss = 5.56535405\n",
      "Iteration 665, loss = 5.64115022\n",
      "Iteration 666, loss = 5.86597685\n",
      "Iteration 667, loss = 5.69565972\n",
      "Iteration 668, loss = 5.53912631\n",
      "Iteration 669, loss = 5.52406028\n",
      "Iteration 670, loss = 5.64170638\n",
      "Iteration 671, loss = 5.55399448\n",
      "Iteration 672, loss = 5.52802071\n",
      "Iteration 673, loss = 5.70577797\n",
      "Iteration 674, loss = 5.62191532\n",
      "Iteration 675, loss = 5.53548268\n",
      "Iteration 676, loss = 5.61587746\n",
      "Iteration 677, loss = 5.59620010\n",
      "Iteration 678, loss = 5.50788276\n",
      "Iteration 679, loss = 5.50579715\n",
      "Iteration 680, loss = 5.57036423\n",
      "Iteration 681, loss = 5.54620147\n",
      "Iteration 682, loss = 5.57399687\n",
      "Iteration 683, loss = 5.78729935\n",
      "Iteration 684, loss = 5.75980557\n",
      "Iteration 685, loss = 5.56440331\n",
      "Iteration 686, loss = 5.56358034\n",
      "Iteration 687, loss = 5.60325058\n",
      "Iteration 688, loss = 5.56043295\n",
      "Iteration 689, loss = 5.51835894\n",
      "Iteration 690, loss = 5.53067821\n",
      "Iteration 691, loss = 5.63345731\n",
      "Iteration 692, loss = 5.65057719\n",
      "Iteration 693, loss = 5.54881558\n",
      "Iteration 694, loss = 5.54192353\n",
      "Iteration 695, loss = 5.54438413\n",
      "Iteration 696, loss = 5.53638653\n",
      "Iteration 697, loss = 5.54644190\n",
      "Iteration 698, loss = 5.93704919\n",
      "Iteration 699, loss = 5.56283950\n",
      "Iteration 700, loss = 5.42954895\n",
      "Iteration 701, loss = 5.44653772\n",
      "Iteration 702, loss = 5.50923422\n",
      "Iteration 703, loss = 5.54139852\n",
      "Iteration 704, loss = 5.54493113\n",
      "Iteration 705, loss = 5.73569509\n",
      "Iteration 706, loss = 5.54417225\n",
      "Iteration 707, loss = 5.44739744\n",
      "Iteration 708, loss = 5.44184470\n",
      "Iteration 709, loss = 5.49922206\n",
      "Iteration 710, loss = 5.54937658\n",
      "Iteration 711, loss = 5.56423305\n",
      "Iteration 712, loss = 5.52357904\n",
      "Iteration 713, loss = 5.54547707\n",
      "Iteration 714, loss = 5.56254952\n",
      "Iteration 715, loss = 5.56051871\n",
      "Iteration 716, loss = 5.50537325\n",
      "Iteration 717, loss = 5.56668796\n",
      "Iteration 718, loss = 5.60331636\n",
      "Iteration 719, loss = 5.43128502\n",
      "Iteration 720, loss = 5.50187575\n",
      "Iteration 721, loss = 5.43911292\n",
      "Iteration 722, loss = 5.51775955\n",
      "Iteration 723, loss = 5.46214843\n",
      "Iteration 724, loss = 5.52601277\n",
      "Iteration 725, loss = 5.64991054\n",
      "Iteration 726, loss = 5.61698091\n",
      "Iteration 727, loss = 5.45612273\n",
      "Iteration 728, loss = 5.38199482\n",
      "Iteration 729, loss = 5.48807844\n",
      "Iteration 730, loss = 5.46854640\n",
      "Iteration 731, loss = 5.43655456\n",
      "Iteration 732, loss = 5.47945873\n",
      "Iteration 733, loss = 5.59095226\n",
      "Iteration 734, loss = 5.71860688\n",
      "Iteration 735, loss = 5.42122354\n",
      "Iteration 736, loss = 5.39582181\n",
      "Iteration 737, loss = 5.45310462\n",
      "Iteration 738, loss = 5.38581797\n",
      "Iteration 739, loss = 5.48611286\n",
      "Iteration 740, loss = 5.54629038\n",
      "Iteration 741, loss = 5.45687202\n",
      "Iteration 742, loss = 5.57385581\n",
      "Iteration 743, loss = 5.54490086\n",
      "Iteration 744, loss = 5.41399876\n",
      "Iteration 745, loss = 5.45745982\n",
      "Iteration 746, loss = 5.42931631\n",
      "Iteration 747, loss = 5.51551674\n",
      "Iteration 748, loss = 5.51945555\n",
      "Iteration 749, loss = 5.47520069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 750, loss = 5.41327579\n",
      "Iteration 751, loss = 5.41749455\n",
      "Iteration 752, loss = 5.38256998\n",
      "Iteration 753, loss = 5.39358615\n",
      "Iteration 754, loss = 5.59317260\n",
      "Iteration 755, loss = 5.50320089\n",
      "Iteration 756, loss = 5.69672857\n",
      "Iteration 757, loss = 5.58003552\n",
      "Iteration 758, loss = 5.40335398\n",
      "Iteration 759, loss = 5.36531618\n",
      "Iteration 760, loss = 5.35016063\n",
      "Iteration 761, loss = 5.34481269\n",
      "Iteration 762, loss = 5.57379872\n",
      "Iteration 763, loss = 5.47041940\n",
      "Iteration 764, loss = 5.42934239\n",
      "Iteration 765, loss = 5.38616150\n",
      "Iteration 766, loss = 5.38655220\n",
      "Iteration 767, loss = 5.37728549\n",
      "Iteration 768, loss = 5.47582555\n",
      "Iteration 769, loss = 5.48688716\n",
      "Iteration 770, loss = 5.46752285\n",
      "Iteration 771, loss = 5.36341147\n",
      "Iteration 772, loss = 5.45016732\n",
      "Iteration 773, loss = 5.36792145\n",
      "Iteration 774, loss = 5.31353667\n",
      "Iteration 775, loss = 5.36155576\n",
      "Iteration 776, loss = 5.49523475\n",
      "Iteration 777, loss = 5.53256790\n",
      "Iteration 778, loss = 5.55875180\n",
      "Iteration 779, loss = 5.51414486\n",
      "Iteration 780, loss = 5.30133677\n",
      "Iteration 781, loss = 5.27516895\n",
      "Iteration 782, loss = 5.40942036\n",
      "Iteration 783, loss = 5.39700754\n",
      "Iteration 784, loss = 5.37929748\n",
      "Iteration 785, loss = 5.49808418\n",
      "Iteration 786, loss = 5.46814375\n",
      "Iteration 787, loss = 5.41433281\n",
      "Iteration 788, loss = 5.31999925\n",
      "Iteration 789, loss = 5.33590975\n",
      "Iteration 790, loss = 5.41943396\n",
      "Iteration 791, loss = 5.58207164\n",
      "Iteration 792, loss = 5.41947021\n",
      "Iteration 793, loss = 5.40768269\n",
      "Iteration 794, loss = 5.37544605\n",
      "Iteration 795, loss = 5.44324627\n",
      "Iteration 796, loss = 5.38262160\n",
      "Iteration 797, loss = 5.33298004\n",
      "Iteration 798, loss = 5.31902245\n",
      "Iteration 799, loss = 5.32746247\n",
      "Iteration 800, loss = 5.38503263\n",
      "Iteration 801, loss = 5.42783714\n",
      "Iteration 802, loss = 5.34503745\n",
      "Iteration 803, loss = 5.27470781\n",
      "Iteration 804, loss = 5.32071470\n",
      "Iteration 805, loss = 5.34830131\n",
      "Iteration 806, loss = 5.37527830\n",
      "Iteration 807, loss = 5.40812890\n",
      "Iteration 808, loss = 5.37676814\n",
      "Iteration 809, loss = 5.33411228\n",
      "Iteration 810, loss = 5.29708746\n",
      "Iteration 811, loss = 5.42367116\n",
      "Iteration 812, loss = 5.46775577\n",
      "Iteration 813, loss = 5.52684153\n",
      "Iteration 814, loss = 5.33048324\n",
      "Iteration 815, loss = 5.34283218\n",
      "Iteration 816, loss = 5.30296495\n",
      "Iteration 817, loss = 5.20249751\n",
      "Iteration 818, loss = 5.30809626\n",
      "Iteration 819, loss = 5.36671618\n",
      "Iteration 820, loss = 5.37974197\n",
      "Iteration 821, loss = 5.33794688\n",
      "Iteration 822, loss = 5.33808410\n",
      "Iteration 823, loss = 5.25359480\n",
      "Iteration 824, loss = 5.27139385\n",
      "Iteration 825, loss = 5.26995677\n",
      "Iteration 826, loss = 5.39029701\n",
      "Iteration 827, loss = 5.39941077\n",
      "Iteration 828, loss = 5.39176371\n",
      "Iteration 829, loss = 5.35448724\n",
      "Iteration 830, loss = 5.34724884\n",
      "Iteration 831, loss = 5.25649357\n",
      "Iteration 832, loss = 5.34765220\n",
      "Iteration 833, loss = 5.41504197\n",
      "Iteration 834, loss = 5.56526146\n",
      "Iteration 835, loss = 5.24783311\n",
      "Iteration 836, loss = 5.23774703\n",
      "Iteration 837, loss = 5.22657620\n",
      "Iteration 838, loss = 5.33006308\n",
      "Iteration 839, loss = 5.33970482\n",
      "Iteration 840, loss = 5.32996170\n",
      "Iteration 841, loss = 5.33496815\n",
      "Iteration 842, loss = 5.41182662\n",
      "Iteration 843, loss = 5.39262957\n",
      "Iteration 844, loss = 5.26967666\n",
      "Iteration 845, loss = 5.29208867\n",
      "Iteration 846, loss = 5.34100698\n",
      "Iteration 847, loss = 5.24020872\n",
      "Iteration 848, loss = 5.41461702\n",
      "Iteration 849, loss = 5.21179364\n",
      "Iteration 850, loss = 5.26902855\n",
      "Iteration 851, loss = 5.29276583\n",
      "Iteration 852, loss = 5.33420986\n",
      "Iteration 853, loss = 5.31279881\n",
      "Iteration 854, loss = 5.18784265\n",
      "Iteration 855, loss = 5.15842733\n",
      "Iteration 856, loss = 5.21584695\n",
      "Iteration 857, loss = 5.27476705\n",
      "Iteration 858, loss = 5.40980823\n",
      "Iteration 859, loss = 5.38491170\n",
      "Iteration 860, loss = 5.37669735\n",
      "Iteration 861, loss = 5.25986658\n",
      "Iteration 862, loss = 5.20126177\n",
      "Iteration 863, loss = 5.27558893\n",
      "Iteration 864, loss = 5.28311419\n",
      "Iteration 865, loss = 5.31223904\n",
      "Iteration 866, loss = 5.28508736\n",
      "Iteration 867, loss = 5.30124168\n",
      "Iteration 868, loss = 5.21729815\n",
      "Iteration 869, loss = 5.14945007\n",
      "Iteration 870, loss = 5.17106581\n",
      "Iteration 871, loss = 5.37248756\n",
      "Iteration 872, loss = 5.34926820\n",
      "Iteration 873, loss = 5.26897013\n",
      "Iteration 874, loss = 5.21343902\n",
      "Iteration 875, loss = 5.21925627\n",
      "Iteration 876, loss = 5.27322728\n",
      "Iteration 877, loss = 5.24305171\n",
      "Iteration 878, loss = 5.27058476\n",
      "Iteration 879, loss = 5.32556184\n",
      "Iteration 880, loss = 5.27055570\n",
      "Iteration 881, loss = 5.28218757\n",
      "Iteration 882, loss = 5.22806094\n",
      "Iteration 883, loss = 5.23674859\n",
      "Iteration 884, loss = 5.37258420\n",
      "Iteration 885, loss = 5.18204944\n",
      "Iteration 886, loss = 5.17278735\n",
      "Iteration 887, loss = 5.27399531\n",
      "Iteration 888, loss = 5.27382269\n",
      "Iteration 889, loss = 5.28544480\n",
      "Iteration 890, loss = 5.21049170\n",
      "Iteration 891, loss = 5.30516962\n",
      "Iteration 892, loss = 5.20803002\n",
      "Iteration 893, loss = 5.20567078\n",
      "Iteration 894, loss = 5.21475776\n",
      "Iteration 895, loss = 5.27271747\n",
      "Iteration 896, loss = 5.30471772\n",
      "Iteration 897, loss = 5.12657139\n",
      "Iteration 898, loss = 5.25131548\n",
      "Iteration 899, loss = 5.22036917\n",
      "Iteration 900, loss = 5.16812063\n",
      "Iteration 901, loss = 5.17985524\n",
      "Iteration 902, loss = 5.40234347\n",
      "Iteration 903, loss = 5.21347817\n",
      "Iteration 904, loss = 5.21818477\n",
      "Iteration 905, loss = 5.29070822\n",
      "Iteration 906, loss = 5.08218125\n",
      "Iteration 907, loss = 5.14599229\n",
      "Iteration 908, loss = 5.11542651\n",
      "Iteration 909, loss = 5.22532985\n",
      "Iteration 910, loss = 5.32611691\n",
      "Iteration 911, loss = 5.30220322\n",
      "Iteration 912, loss = 5.33635205\n",
      "Iteration 913, loss = 5.14892104\n",
      "Iteration 914, loss = 5.05011508\n",
      "Iteration 915, loss = 5.20418656\n",
      "Iteration 916, loss = 5.38920630\n",
      "Iteration 917, loss = 5.25701337\n",
      "Iteration 918, loss = 5.09668264\n",
      "Iteration 919, loss = 5.18068800\n",
      "Iteration 920, loss = 5.17787267\n",
      "Iteration 921, loss = 5.29516043\n",
      "Iteration 922, loss = 5.45450685\n",
      "Iteration 923, loss = 5.16140880\n",
      "Iteration 924, loss = 5.06796937\n",
      "Iteration 925, loss = 5.16672326\n",
      "Iteration 926, loss = 5.29734009\n",
      "Iteration 927, loss = 5.27662860\n",
      "Iteration 928, loss = 5.19189881\n",
      "Iteration 929, loss = 5.06865498\n",
      "Iteration 930, loss = 5.07202766\n",
      "Iteration 931, loss = 5.08801331\n",
      "Iteration 932, loss = 5.25505608\n",
      "Iteration 933, loss = 5.28757782\n",
      "Iteration 934, loss = 5.20324126\n",
      "Iteration 935, loss = 5.33770352\n",
      "Iteration 936, loss = 5.15150336\n",
      "Iteration 937, loss = 5.04680686\n",
      "Iteration 938, loss = 5.09252384\n",
      "Iteration 939, loss = 5.15761816\n",
      "Iteration 940, loss = 5.16592407\n",
      "Iteration 941, loss = 5.22459512\n",
      "Iteration 942, loss = 5.22738127\n",
      "Iteration 943, loss = 5.21520937\n",
      "Iteration 944, loss = 5.21749763\n",
      "Iteration 945, loss = 5.08870999\n",
      "Iteration 946, loss = 5.16208046\n",
      "Iteration 947, loss = 5.25464091\n",
      "Iteration 948, loss = 5.17280511\n",
      "Iteration 949, loss = 5.04210265\n",
      "Iteration 950, loss = 5.06382108\n",
      "Iteration 951, loss = 5.26809139\n",
      "Iteration 952, loss = 5.14287443\n",
      "Iteration 953, loss = 5.16746718\n",
      "Iteration 954, loss = 5.25539331\n",
      "Iteration 955, loss = 5.13954818\n",
      "Iteration 956, loss = 5.29628855\n",
      "Iteration 957, loss = 5.15456858\n",
      "Iteration 958, loss = 5.03404778\n",
      "Iteration 959, loss = 5.06042135\n",
      "Iteration 960, loss = 5.15873312\n",
      "Iteration 961, loss = 5.22755294\n",
      "Iteration 962, loss = 5.12362795\n",
      "Iteration 963, loss = 5.13282496\n",
      "Iteration 964, loss = 5.05968051\n",
      "Iteration 965, loss = 5.12866779\n",
      "Iteration 966, loss = 5.15350932\n",
      "Iteration 967, loss = 5.49775940\n",
      "Iteration 968, loss = 5.43480055\n",
      "Iteration 969, loss = 5.15762958\n",
      "Iteration 970, loss = 5.02207154\n",
      "Iteration 971, loss = 5.11025203\n",
      "Iteration 972, loss = 5.03035903\n",
      "Iteration 973, loss = 4.98170248\n",
      "Iteration 974, loss = 5.07839434\n",
      "Iteration 975, loss = 5.07378602\n",
      "Iteration 976, loss = 5.28926965\n",
      "Iteration 977, loss = 5.27837036\n",
      "Iteration 978, loss = 5.10201841\n",
      "Iteration 979, loss = 5.07270200\n",
      "Iteration 980, loss = 5.00897631\n",
      "Iteration 981, loss = 5.09622207\n",
      "Iteration 982, loss = 5.13675594\n",
      "Iteration 983, loss = 5.07740848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(256,256,256,256,256,256,256,256,256,256,256,256), random_state=1, max_iter=8000, learning_rate_init=0.001, verbose=True, tol=-1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs_bit, primes_bit, test_size=0.95, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(clf.predict(inputs_bit[0:30]),np.array(1 << np.arange(bit_count)).reshape(bit_count,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = clf.predict(X_test)\n",
    "classification_report(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
